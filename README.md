# guardian

This is a project focused on process automatization from web data sources. It carries ETL routines within, and integrations with external tools and platforms.

It is based on OOP so that it can offer a simpler logic to it's code.

It has multiple modules containing sets of codes with direct relationship to the module name and objective. And the main classes of the project are based in a hierarchicle structure.

### Main Components:

* ETL: Using mainly pandas and other tools;
* Integrations: Integrates services and platforms such as Snowflake, Azure blob containers, Google Sheets and others;
* Automatization: It allows us to schedules routines to extract data from web services as Gen.te (LG), Metabase, Locus, Zendesk  and Google Sheets. While also allowing the sharing and transformations of data from Snowflake through specific daily routines.

## Overview

The table below gives an overview of how the automatizations are operating. Observation: There are some reports that are generated by ETL process, not being in it's orginal format.

<table><tr><th><b>Project</b></th><th><b>Concept</b></th><th><b>Quantity of processes</b></th><th><b>Type</b></th></tr>
<tr><td rowspan="14">guardian</td><td rowspan="14">The project contains automatization and etl routines that supports the data vizualization tools and workflows of teams from both MX and BR and the sharing of data for Justo partners</td><td rowspan="6">+96 Report extrations</td><td>+59 Metabase</td></tr>
<tr><td>+25 Google Sheets</td></tr>
<tr><td>+11 Zendesk</td></tr>
<tr><td>1 Playvox</td></tr>
<tr><td>1 Locus</td></tr>
<tr><td>2 Extranet </td></tr>
<tr><td rowspan="8">20 Report generation - ETL</td><td>APIs data to CSV </td></tr>
<tr><td>CSV to GSheets</td></tr>
<tr><td>CSVs to CSV (data manipulation)</td></tr>
<tr><td>CSVs to Blob Container</td></tr>
<tr><td>Snowflake to GSheets</td></tr>
<tr><td>Snowflake to Blob Container </td></tr>
<tr><td>Snowflake to SFTP </td></tr> 
<tr><td>Snowflake to API Endpoints </td></tr> 
</table>

## **Report Extractions List**
A detailed list of all the extractions being executed daily and it's sources can be found in this documentation https://justo.atlassian.net/wiki/spaces/DS1/pages/2486206465/Automatizations+Currently+Active+BR+-+2023-11-03.


## **Modules description**
### `datetimetools`

#### Objective
This module objective is to streamline datetime operations for data engineering tasks. It includes classes for precise time tracking, versatile date formatting for various applications, and utility functions to simplify common datetime manipulations. These tools are designed to support scheduling, reporting, and data processing in a more efficient and standardized manner.

#### Features
- **Time Tracking:** Precisely measures execution durations.
- **Date Formatting:** Provides a diverse range of properties for formatting and calculating dates, crucial for data processing and reporting tasks that require specific date formats or calculations.
- **Utility Functions:** Simplifies complex datetime manipulations, enhancing code readability and efficiency in data processing workflows.

#### Classes and Functions

##### Class: `Timer`
- **Purpose:** To track the duration of code execution segments.
- **Methods:**
  - `start_count()`: Initializes the timer, marking the start of an operation.
  - `finish_count()`: Stops the timer, calculates, and return the elapsed time in minutes and seconds.

##### Class: `Metabase_date_format`
- **Purpose:** To provide specifically formatted date strings for Metabase integration.
- **Selective Property Highlights::**
  - `d_minus_one`: Returns yesterday's date formatted as YYYY-MM-DD.
  - `today`: Returns today's date in YYYY-MM-DD format.
  - `current_month_first_day`: Returns the first day of the current month.
  - `current_month_last_day`: Returns the last day of the current month.

##### Class: `Alternative_date_variables`
- **Purpose:** Offers alternative date formats for reporting and visualization.
- **Selective Property Highlights:**
  - `today_dd_mm_yyyy_with_hyphen`: Returns today's date formatted as DD-MM-YYYY.
  - `yesterday_slash_separated_mm_dd_yyyy`: Returns yesterday's date in MM/DD/YYYY format.

#### Usage Example

```python
from datetimetools import Timer, Metabase_date_format

# Example: Timing a function execution
timer = Timer()
timer.start_count()
# Place the function or code to time here
timer.finish_count()
print(f"Elapsed time: {timer.minutes} minutes and {timer.seconds} seconds.")

# Example: Formatting a date for Metabase
metabase_formatter = Metabase_date_format()
print(f"Yesterday's date for Metabase: {metabase_formatter.yesterday}")
```
---------------------------------------------------------------------------------------------------------
### `etltools`

#### Objective
This module enhances data processing and integration capabilities, focusing on file transformations, data extraction from databases, and data loading into various formats and systems, like Google Sheets or local files. It's designed to facilitate efficient data handling and analysis.

#### Features
- **Data Extraction:** Extracts data from Snowflake into DataFrames for advanced analysis.
- **File Transformation:** Converts Dataframs derived from Snowflake to local files.
- **Data Loading:** Supports data loading from DataFrames to Google Sheets for analysis.

#### Classes and Functions

##### Class: `GSheetDataLoader`
- **Purpose:** Loads DataFrame data into Google Sheets.
- **Method:** 
    - `load_to_gsheets()`: Loads data into a specified Google Sheet, clearing the sheet first, then updating it with new data.

##### Class: `FileToGSheets` (Inherits `GSheetDataLoader`)
- **Purpose:** Reads and uploads file data to Google Sheets.
- **Methods:** 
    - `read_file()`: Reads data from a file (either CSV or Excel) into a pandas DataFrame.
    - `prepare_data()`: If specified, removes duplicate rows from the data to ensure data uniqueness.
    - `force_load()`: Reads and prepares data from a file, then loads it to Google Sheets, combining the processes of reading, preparing, and loading data.

##### Class: `PhatomReport`
- **Purpose:** Serves as a virtual representation for report management within automated processes, enabling seamless integration and handling within data processing workflows. It facilitates the specification of report attributes for functions requiring report information, enhancing the adaptability and efficiency of data management strategies.
- **Attributes:**
    - `name`: Specifies the name of the report, allowing for easy identification and referencing
    - `default_destination_path`: Defines the path(s) where the report file(s) should be saved. This can be a single path or a list of paths, depending on where the report needs to be accessible from.

##### Class: `SnowFlakeToDF`
- **Purpose:** Facilitates data extraction from Snowflake databases into pandas DataFrames for analysis and further manipulation. It supports connecting to Snowflake, executing queries to fetch data, and optionally saving the results to CSV format.
- **Methods:** 
    - `connect_to_sf()`: Establishes a connection to Snowflake using predefined credentials, enabling query execution.
    - `create_df()`: Executes a stored SQL query on the connected Snowflake database and stores the results in a pandas DataFrame for further analysis or manipulation.
    - `df_to_csv(file, index, separator)`: Saves the DataFrame generated from Snowflake data into a CSV file, allowing for customization of the file's index presence and column separator.

#### Usage Example

```python
from your_module_name import FileTransformer, GSheetDataLoader, SnowFlakeToDF

# Load data from a DataFrame to a specific Google Sheet
loader = GSheetDataLoader('google_sheet_key_here', 'worksheet_name_here')
# Assuming 'dataframe' is a pandas DataFrame you've prepared
loader.data = dataframe
loader.load_to_gsheets()

# Extract data from Snowflake into a DataFrame and then export it to a CSV file
sf_to_df = SnowFlakeToDF("SELECT * FROM your_table")
df = sf_to_df.create_df()
if df is not None:
    sf_to_df.df_to_csv('path/to/exported_file.csv')

# Note: This example assumes that the necessary data and connections are correctly set up.
```
-----------------------------------------------------------------------------------------------------------
### `main`

#### Objective
Designed for task management, this module centralize and streamline the execution of various report generation tasks across multiple data sources and platforms. Working as an orchestrator of all events that occur in the project. It facilitates the scheduling of report generation, error handling, and ensures systematic data handling through categorized report groups for efficient ETL processes.
 
#### Features
- **Automated Scheduling:** Timely execution of report tasks, adhering to a predefined schedule.
- **Error Handling:** Notifications and retries for errors, thereby maintaining the integrity and reliability of data workflows.
- **Report Management:** Systematic data handling through categorized report groups, considering diffent web data platforms (like Metabase, Google Sheets, etc.).

#### Classes and Functions

##### Class: `ReportGroup`
- **Purpose:** Organizes reports from different sources into groups. So that they can extracted one after another if needed.
- **Attributes:** Groups like `last_mile`, `onmaps`, categorized by teamm names or project description.

##### Class: `Manager`
- **Purpose:** Manages the execution of report generation tasks, facilitating systematic error handling and notifications.
- **Selective Method Highlights:** 
  - `set_reports(metabase_list)`: Populates the list of reports extraction to be executed.
  - `errors_reset()`: Clears the list of errored reports, preparing for a new execution cycle.
  - `switch(report_set)`: Executes specified report extraction set or a particular report extraction.
  - `run_and_verify(report)`: Executes a report and verifies its completion; if errors occur, it handles them according to the report's settings.

##### Utility Functions
- **`etl_routines()`:** Executes ETL and validation scripts.
- **`workdays()`:** Schedules tasks for workdays.

#### Usage Example

```python
from main import ReportGroup, Manager

# Initialize and run report group
managermb= Manager()
switch = managermb.switch
switch(report_grop)

# Schedule tasks
workdays()
```
----------------------------------------------------------------------------------------------------------
### `mainruntime`

#### Objective

This module streamlines the process of extracting data from various platforms, including Google Sheets, Metabase, Locus, Playvox, Zendesk, and internal systems. It's designed to automate and simplify data extraction, definining the general workflow of all web data extractions and ensuring efficient retrieval of information necessary for reporting and analysis.

#### Features

- **Automated Extractions:** Facilitates data retrieval from multiple sources, and defines it's general workflow.
- **Error Handling:** Implements robust error checking and notifications.
- **Browser Automation:** Utilizes browser interactions for data downloads.

#### Classes and Functions

##### Class: `ProcessRunner`

- **Purpose:** Manages the automated extraction of data from specified links.
- **Methods:**
    -   `download_gsheets()`: Automates data download from Google Sheets.
    -   `download_metabase()`: Extracts reports from Metabase.
    -   `download_lg()`: Retrieves data from LG's platform.
    -   `download_playvox()`: Downloads data from Playvox.
    -   `download_zendesk()`: Extracts reports from Zendesk.
    -   `download_extranet()`: Handles data retrieval from Extranet platforms.
    -   `process_sorting()`: Determines the source platform and initiates the appropriate download method based on report classification.

#### Usage Example

```python
from extractiontools import ProcessRunner
from reports import MReport01 # Example report class

# Initialize ProcessRunner with a report
process_runner = ProcessRunner(MReport01)

# Execute the appropriate download method based on the report's source
process_runner.process_sorting()

# Check for errors and complete the process
process_runner.check_error()`
```
----------------------------------------------------------------------------------------------------------
### `plconfig`

#### Objective

This module manages configuration settings, including access parameters, language preferences, and file paths. It's crucial for customizing the environment and ensuring secure access to data sources and APIs.


#### Features

- **Locale Management:** Adjusts settings based on user locale.
- **Path Configuration:** Sets and retrieves paths for file operations.
- **Secure Information Handling:** Manages sensitive data securely.

#### Classes and Functions

##### Class: `FolderUpperCleaner`

- **Purpose:** Cleans specified directories to maintain organization.
- **Methods:**
    -   `delete(files_list)`: Removes files from a list.
    -   `last_mile_set()`: Cleans up directories related to 'last mile' reports.
    -   `clean_downloads()`: Empties the downloads folder.
    -   `remove_call(process_name)`: Executes cleaning based on process name.

##### Class: `MachineInfo`

- **Purpose:** Retrieves machine and user-specific information.
- **Selective Attributes Highlights:** 
    - `windll`: Utilizes the Windows DLL (Dynamic Link Library) to access system-level calls, enabling interactions with Windows APIs for system information retrieval. 
    - `win_user`: Retrieves the username of the current Windows user, ensuring user-specific paths and settings can be applied.
    - `win_user_with_path_string`: Constructs a path string that includes the Windows username, which can be used for accessing user-specific directories or resources.
    - `lang`: Determines the system's user interface language through the Windows API, allowing the application to adapt its operations or output to match the user's language settings.

##### Class: `SecInfo`

- **Purpose:** Manages access to secured information like API keys, passwords and sensitive information.
- **Method:**
    -   `_read_secret(filepath)`: Reads secrets from a specified file.

#### Usage Example
```python
from configtools import FolderUpperCleaner, MachineInfo, SecInfo

# Initialize cleaner and perform directory cleaning
cleaner = FolderUpperCleaner()
cleaner.last_mile_set()

# Retrieve machine information
machine_info = MachineInfo()
print(machine_info.win_user)

# Access secure information
sec_info = SecInfo()
print(sec_info.snf_pss)
```
----------------------------------------------------------------------------------------------------------
### `verifiers`

#### Objective

This module facilitates Slack bot configuration, alerting, and logging, integrating environmental settings and system-specific configurations for efficient communication and error tracking.

#### Features

- **Slack Integration:** Enables messaging and alerts through Slack channels.
- **File Verification:** Checks for recent modifications to ensure data freshness.
- **Logging: Maintains** logs for process tracking and error reporting.

#### Classes and Functions

##### Class: `ProcessRegistrant`

- **Purpose:** Manages logging and alerting for process execution statuses.
- **Methods:**
    -   `call_critical()`: Sends critical error alerts.
    -   `register_info(info)`: Logs informational messages.
    -   `register_error(error)`: Logs error messages and notifies via Slack.

##### Utility Functions

-   `inst_verify(list_of_paths, channel_name)`: Verifies file modifications and sends alerts for outdated files.
-   `send_message(message, channel_name)`: Posts messages to specified Slack channels.

#### Usage Example

```python
from notificationtools import send_message, ProcessRegistrant

# Sending a simple message to a Slack channel
send_message("Hello, world!", "general")

# Registering an information log
process_registrant = ProcessRegistrant()
process_registrant.register_info("Process completed successfully.")
```
----------------------------------------------------------------------------------------------------------
### `webactions`

#### Objective

This module is engineered to automate interactions with web platforms for data extraction purposes. It supports automated login procedures, data retrieval, and handling of cookies for platforms like Metabase, Google Sheets, Zendesk, and LG.

#### Features

- **Automated Web Interactions:** Manages browser-based tasks for data extraction.
- **Cookie Management:** Handles cookies for session persistence across platforms.
- **Data Extraction:** Automates the download process from various data sources.

#### Classes and Functions

##### Class: `BrowserInitiator`

- **Purpose:** Initiates and manages browser sessions for data extraction.
- **Selective Methods Highlights:**
    -   `start_browser(p)`: Launches a browser session with predefined settings.
    -   `load_cookies(method)`: Loads cookies into the browser session for authentication.
    -   `close_browser()`: Closes the browser session after operations are complete.

##### Class: `Authenticator`

- **Purpose:** Handles login and authentication processes for web-based services.
- **Selective Methods Highlights:**
    -   `authenticate_metabase()`, `authenticate_gsheets()`, `authenticate_zendesk()`, `authenticate_lg()`: Perform login procedures for respective platforms.
    -   `fill(value)`: Fills in text details.
    -   `click_to_log_in(button)`: Clicks login buttons and navigates through authentication flows.

##### Class: `Extractor`

- **Purpose:** Executes data extraction from web sources.
- **Methods:**
    -   `extract_mb_data()`, `extract_gs_data()`, `extract_zk_data()`, `generate_lg_data()`: Retrieve data from Metabase, Google Sheets, Zendesk, and LG respectively.

#### Usage Example

```python
from webactions import Authenticator, Extractor

# Setting up Authenticator for Google Sheets
authenticator = Authenticator(BrowserInitiator())
authenticator.authenticate_gsheets()

# Extracting data from Google Sheets
extractor = Extractor(time_limit, sheet_link, destination_paths)
extractor.extract_gs_data()
```
----------------------------------------------------------------------------------------------------------
### `reports`


#### Objective

This module defines reports for automation across various platforms, including Metabase, Zendesk, Google Sheets, Locus, and Extranet. It ensures structured management of data extraction tasks tailored to specific report requirements. Utilizing object-oriented principles, it introduces a base class to streamline common attributes, thus reducing redundancy and enhancing maintainability.

#### Features
- **Automated Data Handling:** Streamlines processes for efficient data retrieval.
- **Report Definitions:** Specifies configurations for automated report generation.
- **Inheritance-Driven Design:** Leverages a base class to share common report attributes, ensuring code reusability and consistency.

#### Classes and Functions

##### Class: `ReportBase`

- **Purpose:** Serves as the foundation for all report classes, encapsulating shared attributes and methods.
- **Common Attributes:** 
    -   `time_formats`: Provides access to common time formatting utilities.
    -   `metabase_date_format`: Offers standardized date formats for integration with Metabase.
    -   `machine_info`: Contains information about the machine environment to adapt paths and settings accordingly.
    -   `alternative_date_variables`: Includes alternative date formats for flexibility in report generation.
    -   `instant_notification`: A boolean flag to enable or disable immediate notifications upon report generation or failure.
    -   `channel`: Specifies the communication channel for report notifications.
    -   `forced_try`: Indicates whether to force retrying the report generation in case of failure.

##### Derived Classes (Examples: `MReport02`, `ZReport05`, `AReport01`)

- **Purpose:** Each derived class represents a specific report, inheriting common attributes from `ReportBase` and adding or overriding specific details. The initial letter of each class name indicates the platform the report is from (e.g., M for Metabase, Z for Zendesk), facilitating filtering and identification by other processes.


- **Custom Attributes:** Each class specifies unique values for attributes such as `name`, `original_file_name`, `default_destination_path`, `link`, and `limit_time`, tailored to the report's requirements.

#### Usage Example

```python
# Define a report for Metabase
class MReport02(ReportBase):
    instant_notification = True
    name = 'Closed Delivered By Warehouse SD'
    original_file_name = 'closed___delivered_by_warehouse'
    # Define properties for destination path and link specific to this report

# Instantiate and use a report
report = MReport02()
print(report.name)
print(report.link())
```
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
### Comes with Python by Default

The following libraries are part of Python's standard library and are therefore available by default when you install Python:

-   `datetime` (for `datetime`, `timedelta`)
-   `time`
-   `os`
-   `shutil`
-   `json`
-   `logging`
-   `glob`
-   `ctypes`
-   `locale`
-   `pathlib` (for `Path`)

### Need to be Installed Separately

The following libraries are third-party libraries and need to be installed separately, usually using pip:

-   `numpy`
-   `pyodbc`
-   `azure.storage.blob` (for `BlobServiceClient`)
-   `googletrans` (for `Translator`)
-   `pandas`
-   `dateutil` (for `relativedelta`)
-   `gspread`
-   `gspread_dataframe` (as `gd`)
-   `oauth2client` (for `ServiceAccountCredentials`)
-   `schedule`
-   `runpy`
-   `pyautogui`
-   `pyperclip`
-   `pytesseract` (as `pytesseract`)
-   `slack`
-   `dotenv` (for `load_dotenv`)
-   `playwright` (for `sync_playwright`)

If you need to install any of these third-party libraries, you can usually do so using the `pip` command. For example, to install `numpy`, you would use:

```bash
pip install numpy
```
------------------------------------------------------------------------------------------------------------------------------------------------------
## Aditional Processes
### 1\. ETL Routine

Description: ETL routine refers to a set of custom Extract, Transform, and Load (ETL) processes designed to be executed periodically. These routines are crucial for the operational and analytical aspects of data management within the project. They are orchestrated by a utility function named `etl_routines()` located in the "main" module of the project.

Path: `processes and modules/etl_routines/`

Functionality:

-   Extract: The ETL routines begin by extracting data from various sources. These sources can range from databases, APIs, flat files, Snowflake and other data repositories used by the project or external entities.
-   Transform: After extraction, the data undergoes a transformation process. This may involve cleaning, aggregating, merging datasets, and applying business logic to prepare the data for analytical or operational use. The transformation process is tailored to meet the specific requirements of cross-company data platforms and analytical applications, ensuring that the data is in the most useful format and structure for analysis and decision-making.
-   Load: Finally, the transformed data is loaded into target destinations, which could be databases, data warehouses, or other data platforms utilized by the project. This step makes the data accessible for analysis, reporting, and other downstream applications.

Purpose: The primary aim of the ETL routines is to facilitate the integration and utilization of data across different platforms, teams and companies. By automating the extraction, transformation, and loading of data, these routines support analytical processes and enable data-sharing across different destinations.

### 2\. Validations

Description: Validations consist of a series of data checks and validations that are executed periodically. These processes are designed to ensure the accuracy, consistency, and reliability of the data within the system. By applying specific algorithms and rules, the validation processes can identify data anomalies and inconsistencies that need to be addressed.

Path: `processes and modules/validations/`

Functionality:

-   Data Checks: Perform thorough checks on the data to validate its accuracy and consistency. This includes verifying data formats, ranges, and other predefined criteria to ensure the data meets the expected standards.
-   Algorithmic Validation: Utilize specific algorithms to assess the data's integrity and relevance. These algorithms can identify patterns or trends that deviate from the norm, and highlight potential issues that require further investigation.
-   Alerts and Messages: When anomalies or issues are detected, the validation processes can generate custom messages or alerts through slack. These notifications are designed to inform relevant stakeholders or other teams about potential data quality issues, enabling them to take corrective actions or make informed decisions based on the validation results.

Purpose: The validations serve as a proactive measure to maintain the quality and integrity of the data for specific teams, enhancing overall organizational efficiency.
